{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlexNet v2.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Get GPU Device Name (If Available)**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K6iP0H48k7Ku"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIqOe2bmkQLI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "print(f'GPU Device Name: {torch.cuda.get_device_name()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mount Google Drive**"
      ],
      "metadata": {
        "id": "trViB5ilvLUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CPoa4T-faWiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Necesssary Libraries**"
      ],
      "metadata": {
        "id": "sdv-8YM-vPFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U split-folders\n",
        "!pip install -q -U pytorchtools\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "2paG0dODrldS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "LJ0-rc64v1BN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorboard as tb\n",
        "import splitfolders\n",
        "import numpy as np\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "Egbaov1GvgED"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import dataset files**"
      ],
      "metadata": {
        "id": "17Q4RPP3vUmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip zip files of data from google drive\n",
        "!unzip -q \"/content/drive/MyDrive/train_data.zip\" -d \"/content\"\n",
        "!unzip -q \"/content/drive/MyDrive/test_data.zip\" -d \"/content\"\n",
        "\n",
        "# Split train data into train and val set (final result -> train:val:test = 70:10:20)\n",
        "splitfolders.ratio(\"/content/train_data/train_img\", \"/content\",\n",
        "                seed=42, ratio=(0.875, 0.125))\n",
        "\n",
        "# create test folder and import data\n",
        "!mv \"/content/test_data/test_img\" \"/content\"\n",
        "!mv \"/content/test_img\" \"/content/test\"\n",
        "\n",
        "# remove unnecessary files\n",
        "!rm -rf \"train_data\" \"test_data\" \"sample_data\""
      ],
      "metadata": {
        "id": "XQVKRKVxXHS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set device (GPU or CPU)**"
      ],
      "metadata": {
        "id": "EpoJbeO6vrHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "XGqto6Gtvjnp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Augmentation**"
      ],
      "metadata": {
        "id": "qDx369BHM8k0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "    transforms.ColorJitter(brightness=0.5),\n",
        "    transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomEqualize(p=0.5),\n",
        "    transforms.RandomAutocontrast(p=0.5),\n",
        "    transforms.RandomRotation(degrees=(-20,20)),\n",
        "    transforms.CenterCrop(255),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.3826, 0.3692, 0.3057], std=[0.3249, 0.3166, 0.3029])\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_transforms = transforms.Compose(\n",
        "    [\n",
        "    transforms.CenterCrop(255),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.3820, 0.3704, 0.3064], std=[0.3255, 0.3179, 0.3040])\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Qt0eo_7wvlmG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Batch Size**"
      ],
      "metadata": {
        "id": "O7ZJq_YMj814"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set batch size\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "maYWc1nrkBpE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Datasets**"
      ],
      "metadata": {
        "id": "pKnOuDRvjJhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set directories for data\n",
        "TRAIN_DATA_PATH = \"/content/train\"\n",
        "VAL_DATA_PATH = \"/content/val\"\n",
        "TEST_DATA_PATH = \"/content/test\"\n",
        "\n",
        "# load data for each dataset\n",
        "train_data = datasets.ImageFolder(root = TRAIN_DATA_PATH, transform=train_transforms)\n",
        "train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "val_data = datasets.ImageFolder(root = VAL_DATA_PATH, transform=test_transforms)\n",
        "val_data_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "test_data = datasets.ImageFolder(root = TEST_DATA_PATH, transform=test_transforms)\n",
        "test_data_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)"
      ],
      "metadata": {
        "id": "f1az-jBejOc6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AlexNet 2.0**"
      ],
      "metadata": {
        "id": "OOfKxgyVxZRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNetV2(nn.Module):\n",
        "    def __init__(self, in_channels = 3, hidden_size1 = 2048, num_classes = 10):\n",
        "        super(AlexNetV2, self).__init__()\n",
        "        # CONV layers\n",
        "        self.conv1 = nn.Conv2d(3, 64, (3,3), stride=(1,1), padding=(1,1), bias=False)\n",
        "        self.conv2 = nn.Conv2d(64, 128, (3,3), stride=(1,1), padding=(1,1), bias=False)\n",
        "        self.conv3 = nn.Conv2d(128, 256, (3,3), stride=(1,1), padding=(1,1), bias=False)\n",
        "        self.conv4 = nn.Conv2d(256, 512, (3,3), stride=(1,1), padding=(1,1), bias=False)\n",
        "        self.conv5 = nn.Conv2d(512, 256, (3,3), stride=(1,1), padding=(1,1), bias=False)\n",
        "        \n",
        "        # MAXPOOL layers\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(3,3), stride=(2,2))\n",
        "        \n",
        "        # Batch Norm layers\n",
        "        self.conv1_bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv2_bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3_bn2 = nn.BatchNorm2d(256)\n",
        "        self.conv4_bn2 = nn.BatchNorm2d(512)\n",
        "        self.conv5_bn2 = nn.BatchNorm2d(256)\n",
        "        self.fc1_bn1 = nn.BatchNorm1d(hidden_size1)\n",
        "\n",
        "        # FC layers\n",
        "        self.fc1 = nn.Linear(256*15*15, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, num_classes)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1_bn2(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.conv2_bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.conv3_bn2(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.conv4_bn2(self.conv4(x))))\n",
        "        x = F.relu(self.conv5_bn2(self.conv5(x)))\n",
        "                \n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc1_bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_uniform_(m.weight)\n",
        "\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m_bias, 0)"
      ],
      "metadata": {
        "id": "GQhau9AUxb5f"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameters ,Optimizer, and Loss Function**"
      ],
      "metadata": {
        "id": "kmFPK3qDsMKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001 \n",
        "num_epochs = 20\n",
        "\n",
        "model =  AlexNetV2().to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "s-huy_Q3sQrD"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Structures to Save Data**"
      ],
      "metadata": {
        "id": "f3Eg6Lnqot_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# epoch\n",
        "epoch_num = [x for x in range(1,num_epochs+1)]\n",
        "\n",
        "# training accuracy and loss\n",
        "train_accuracy, total_train_loss = [], []\n",
        "\n",
        "# validation accuracy and loss\n",
        "val_accuracy, total_val_loss = [], []"
      ],
      "metadata": {
        "id": "fYPYRaMRoxJ1"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training (and Val Loop)**"
      ],
      "metadata": {
        "id": "Ks8tucUeoDSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, num_epochs+1):\n",
        "    train_num_correct, train_num_samples = 0, 0\n",
        "    val_num_correct, val_num_samples = 0, 0\n",
        "\n",
        "    # training loop\n",
        "    model.train()\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    tmp_train_loss = []\n",
        "    for imgs, classes in train_data_loader:\n",
        "        imgs, classes = imgs.to(device=device), classes.to(device=device)\n",
        "\n",
        "        # Forward propagation\n",
        "        output = model(imgs)\n",
        "        t_loss = loss_func(output, classes)\n",
        "        tmp_train_loss.append(t_loss.item())\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backprop (Calculating gradients)\n",
        "        t_loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            scores = model(imgs)\n",
        "            _, predictions = scores.max(1)\n",
        "\n",
        "            train_num_correct += (predictions==classes).sum()\n",
        "            train_num_samples += predictions.size(0)\n",
        "\n",
        "    train_acc = (train_num_correct)/(train_num_samples)*100\n",
        "    train_accuracy.append(train_acc.cpu())\n",
        "    tmp_train_loss = np.array(tmp_train_loss)\n",
        "    epoch_train_loss = np.mean(tmp_train_loss)\n",
        "    total_train_loss.append(epoch_train_loss)\n",
        "\n",
        "    # validation loop\n",
        "    model.eval()\n",
        "    tmp_val_loss = []\n",
        "    for imgs, classes in val_data_loader:\n",
        "        imgs, classes = imgs.to(device=device), classes.to(device=device)\n",
        "\n",
        "        # Forward propagation\n",
        "        output = model(imgs)\n",
        "        v_loss = loss_func(output, classes)\n",
        "        tmp_val_loss.append(v_loss.item())\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Backprop\n",
        "        v_loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            scores = model(imgs)\n",
        "            _, predictions = scores.max(1)\n",
        "\n",
        "            val_num_correct += (predictions==classes).sum()\n",
        "            val_num_samples += predictions.size(0)\n",
        "        \n",
        "    val_acc = (val_num_correct)/(val_num_samples)*100\n",
        "    val_accuracy.append(val_acc.cpu())\n",
        "    tmp_val_loss = np.array(tmp_val_loss)\n",
        "    epoch_val_loss = np.mean(tmp_val_loss)\n",
        "    total_val_loss.append(epoch_val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch} | Train % = {train_acc} | Val % = {val_acc} | Train loss = {epoch_train_loss} | Val loss = {epoch_val_loss}')"
      ],
      "metadata": {
        "id": "k7DCZ3FVwH8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Set**"
      ],
      "metadata": {
        "id": "lcojMTWxoMjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_num_correct = 0\n",
        "test_num_samples = 0\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for img, classes in test_data_loader:\n",
        "        img, classes = img.to(device=device), classes.to(device=device)\n",
        "\n",
        "        output = model(img)\n",
        "        _, predictions = output.max(1)\n",
        "        test_num_correct += (predictions == classes).sum()\n",
        "        test_num_samples += predictions.size(0)\n",
        "    \n",
        "    print(f'Test Accuracy ----------------------------> {(test_num_correct)/(test_num_samples)*100:.2f}%')"
      ],
      "metadata": {
        "id": "oVrv1L6PoO6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy Chart (Train and Val)**"
      ],
      "metadata": {
        "id": "_9JtwpAkGcBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epoch_num, train_accuracy, 'r', label=\"train\")\n",
        "plt.plot(epoch_num, val_accuracy, 'b', label=\"val\")\n",
        "plt.title('Accuracy for AlexNet v2.0 for Animals10')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(loc=\"lower right\")\n",
        "#plt.rcParams['figure.figsize'] = [10, 5]\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sG9OykhlvufS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss (Train and Val)**"
      ],
      "metadata": {
        "id": "I-MqkTMkGhPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epoch_num, total_train_loss, 'r', label=\"train\")\n",
        "plt.plot(epoch_num, total_val_loss, 'b', label=\"val\")\n",
        "plt.title('Loss for AlexNet v2.0 for Animals10')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.rcParams['figure.figsize'] = [10, 5]\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5pm9ZSJg84zY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of the model**"
      ],
      "metadata": {
        "id": "gbLohfONDLWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, (3, 255, 255))"
      ],
      "metadata": {
        "id": "DmOd4k0IJjCu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}